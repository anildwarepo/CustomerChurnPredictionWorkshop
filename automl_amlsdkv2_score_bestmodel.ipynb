{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "model_path = os.path.join(\"./artifact_downloads/outputs\", \"mlflow-model\" ,\"model.pkl\")\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_data_orig = pd.read_csv(\"./wa_telco_customer_churn_test_data/WA_Fn-UseC_-Telco-Customer-Churn_Test.csv\")\n",
    "\n",
    "test_data = test_data_orig.drop(\"Churn\", axis=1)\n",
    "test_data[\"Partner\"] = test_data[\"Partner\"].map({\"Yes\": True, \"No\": False})\n",
    "test_data[\"Dependents\"] = test_data[\"Dependents\"].map({\"Yes\": True, \"No\": False})\n",
    "test_data[\"PhoneService\"] = test_data[\"PhoneService\"].map({\"Yes\": True, \"No\": False})\n",
    "test_data[\"PaperlessBilling\"] = test_data[\"PaperlessBilling\"].map({\"Yes\": True, \"No\": False})\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_orig[\"Churn_Prediction\"] = result\n",
    "\n",
    "test_data_orig['Churn_Prediction'] = test_data_orig['Churn_Prediction'].map({True: 'Yes', False: 'No'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
       "       'tenure', 'PhoneService', 'MultipleLines', 'InternetService',\n",
       "       'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n",
       "       'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n",
       "       'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn',\n",
       "       'Churn_Prediction'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      customerID Churn Churn_Prediction\n",
      "0     4376-KFVRS    No               No\n",
      "1     2754-SDJRD    No              Yes\n",
      "2     9917-KWRBE    No               No\n",
      "3     0365-GXEZS    No               No\n",
      "4     9385-NXKDA    No               No\n",
      "...          ...   ...              ...\n",
      "1404  5204-HMGYF    No               No\n",
      "1405  9950-MTGYX    No               No\n",
      "1406  3675-EQOZA    No               No\n",
      "1407  3646-ITDGM    No               No\n",
      "1408  3913-FCUUW    No               No\n",
      "\n",
      "[1409 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "selected_columns = test_data_orig[['customerID', 'Churn', 'Churn_Prediction']]\n",
    "\n",
    "# Display the selected columns\n",
    "print(selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80\n",
      "Precision: 0.65\n",
      "Recall: 0.51\n",
      "F1-Score: 0.57\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming you have a DataFrame called 'test_data_orig'\n",
    "# with 'Churn' representing actual labels ('No' or 'Yes') and 'Churn_Prediction' representing predicted labels ('No' or 'Yes')\n",
    "actual_labels = (test_data_orig['Churn'] == 'Yes').astype(int)  # Convert 'Yes' to 1 and 'No' to 0\n",
    "predicted_labels = (test_data_orig['Churn_Prediction'] == 'Yes').astype(int)  # Convert 'Yes' to 1 and 'No' to 0\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score\n",
    "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "precision = precision_score(actual_labels, predicted_labels)\n",
    "recall = recall_score(actual_labels, predicted_labels)\n",
    "f1 = f1_score(actual_labels, predicted_labels)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy (0.80): An accuracy of 80% indicates that your model correctly predicts whether a customer will churn or not in 80% of the cases. This is a reasonably good starting point, but it's important to consider other metrics due to the potential class imbalance in customer churn problems. For example, if only a small percentage of customers actually churn, a model that predicts \"no churn\" for all customers could still achieve a high accuracy, but it would not be useful.\n",
    "\n",
    "Precision (0.65): Precision of 65% means that when your model predicts a customer will churn, it is correct 65% of the time. This metric is important because you want to avoid incorrectly flagging customers who don't intend to churn as potential churners. A precision of 65% suggests that the model is decent at identifying actual churners without too many false positives.\n",
    "\n",
    "Recall (0.51): Recall of 51% means that your model is able to capture 51% of the actual churn cases. While this isn't extremely high, it's still meaningful because it indicates that the model can identify more than half of the customers who are likely to churn. Recall is important when you want to minimize false negatives and ensure you don't miss potential churners.\n",
    "\n",
    "F1-Score (0.57): The F1-Score of 0.57 is a balanced metric that takes both precision and recall into account. It suggests that the model strikes a balance between identifying actual churners and avoiding false alarms.\n",
    "\n",
    "In customer churn prediction, it's often more important to have a higher recall (to capture as many potential churners as possible) while maintaining a reasonable precision (to avoid unnecessarily alarming non-churners). However, the balance between precision and recall depends on the specific business goals and costs associated with churn.\n",
    "\n",
    "Consider monitoring these metrics over time, and if possible, compare your model's performance to previous methods or industry benchmarks to assess whether further improvements are needed. Additionally, you might want to perform more advanced analyses, such as feature importance assessment or model tuning, to enhance your churn prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customerID           object\n",
      "gender               object\n",
      "SeniorCitizen         int64\n",
      "Partner                bool\n",
      "Dependents           object\n",
      "tenure                int64\n",
      "PhoneService         object\n",
      "MultipleLines        object\n",
      "InternetService      object\n",
      "OnlineSecurity       object\n",
      "OnlineBackup         object\n",
      "DeviceProtection     object\n",
      "TechSupport          object\n",
      "StreamingTV          object\n",
      "StreamingMovies      object\n",
      "Contract             object\n",
      "PaperlessBilling     object\n",
      "PaymentMethod        object\n",
      "MonthlyCharges      float64\n",
      "TotalCharges         object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Show column types\n",
    "column_types = test_data.dtypes\n",
    "print(column_types)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automlscoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
