{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring using the best model locally - Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training environment in AML compute cluster can be different than the one locally. We need to create a new Conda environment in the repo so that model can be loaded locally. \n",
    "\n",
    "    conda env create -f conda_env.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the model file downloaded using automl_amlsdkv2_training.ipynb. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If AML Compute Instance is used, then the notebook kernel should be set to Python 3.8 - AzureML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "model_path = os.path.join(\"./artifact_downloads/outputs\", \"mlflow-model\" ,\"model.pkl\")\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the test dataset created in automl_amlsdkv2_training.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the test dataset and drop the Churn column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_data_orig = pd.read_csv(\"./wa_telco_customer_churn_test_data/WA_Fn-UseC_-Telco-Customer-Churn_Test.csv\")\n",
    "\n",
    "test_data = test_data_orig.drop(\"Churn\", axis=1)\n",
    "test_data[\"Partner\"] = test_data[\"Partner\"].map({\"Yes\": True, \"No\": False})\n",
    "test_data[\"Dependents\"] = test_data[\"Dependents\"].map({\"Yes\": True, \"No\": False})\n",
    "test_data[\"PhoneService\"] = test_data[\"PhoneService\"].map({\"Yes\": True, \"No\": False})\n",
    "test_data[\"PaperlessBilling\"] = test_data[\"PaperlessBilling\"].map({\"Yes\": True, \"No\": False})\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge prediction results with test data to compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_orig[\"Churn_Prediction\"] = result\n",
    "\n",
    "test_data_orig['Churn_Prediction'] = test_data_orig['Churn_Prediction'].map({True: 'Yes', False: 'No'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = test_data_orig[['customerID', 'Churn', 'Churn_Prediction']]\n",
    "\n",
    "# Display the selected columns\n",
    "print(selected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy, precision, recall, and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming you have a DataFrame called 'test_data_orig'\n",
    "# with 'Churn' representing actual labels ('No' or 'Yes') and 'Churn_Prediction' representing predicted labels ('No' or 'Yes')\n",
    "actual_labels = (test_data_orig['Churn'] == 'Yes').astype(int)  # Convert 'Yes' to 1 and 'No' to 0\n",
    "predicted_labels = (test_data_orig['Churn_Prediction'] == 'Yes').astype(int)  # Convert 'Yes' to 1 and 'No' to 0\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score\n",
    "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "precision = precision_score(actual_labels, predicted_labels)\n",
    "recall = recall_score(actual_labels, predicted_labels)\n",
    "f1 = f1_score(actual_labels, predicted_labels)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy (0.80): An accuracy of 80% indicates that your model correctly predicts whether a customer will churn or not in 80% of the cases. This is a reasonably good starting point, but it's important to consider other metrics due to the potential class imbalance in customer churn problems. For example, if only a small percentage of customers actually churn, a model that predicts \"no churn\" for all customers could still achieve a high accuracy, but it would not be useful.\n",
    "\n",
    "Precision (0.65): Precision of 65% means that when your model predicts a customer will churn, it is correct 65% of the time. This metric is important because you want to avoid incorrectly flagging customers who don't intend to churn as potential churners. A precision of 65% suggests that the model is decent at identifying actual churners without too many false positives.\n",
    "\n",
    "Recall (0.51): Recall of 51% means that your model is able to capture 51% of the actual churn cases. While this isn't extremely high, it's still meaningful because it indicates that the model can identify more than half of the customers who are likely to churn. Recall is important when you want to minimize false negatives and ensure you don't miss potential churners.\n",
    "\n",
    "F1-Score (0.57): The F1-Score of 0.57 is a balanced metric that takes both precision and recall into account. It suggests that the model strikes a balance between identifying actual churners and avoiding false alarms.\n",
    "\n",
    "In customer churn prediction, it's often more important to have a higher recall (to capture as many potential churners as possible) while maintaining a reasonable precision (to avoid unnecessarily alarming non-churners). However, the balance between precision and recall depends on the specific business goals and costs associated with churn.\n",
    "\n",
    "Consider monitoring these metrics over time, and if possible, compare your model's performance to previous methods or industry benchmarks to assess whether further improvements are needed. Additionally, you might want to perform more advanced analyses, such as feature importance assessment or model tuning, to enhance your churn prediction model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automlscoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
